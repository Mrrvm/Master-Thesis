%!TEX root = ../dissertation.tex

\section{Orthogonal Procrustes Problem and Lack of Depth}
\label{cha2:opprandsphere}

Assuming that the image points are obtained from the RGB camera installed on the eye prototype, using the camera model, this 2D points can be converted to corresponding points in space. 

The 3D points can then be used to estimate the rotation executed by the eye. There are several ways to do this, a potential one is \acrfull{oppr}. This algorithm finds the optimal linear transformation between two 3D clouds of points.

Procrustes, the son of Poseidon in Greek mythology, made his victims fit in a wonderful all-fitting bed, either by stretching their limbs, or by cutting them off. Ultimately, he was fitted into his own bed by Theseus.\footnote{Karl Kerenyi, The Heroes of the Greeks, 1959:223, noting pseudo-ApollodorusDiodorus Siculus, 4.59.5.} This is similar to the analysis held here, there are 3 elements to Procrustes's problem: $M_2$, the bed, $M_1$, the unlucky adventurer, and T, the fitting treatment. The solution to the problem is a matrix, T, that minimizes
\begin{equation}
\label{sec2:eq:procrustes}
\left \| M_2 - TM_1  \right \|^2 ,
\end{equation}
which transforms $M_1$ into $M_2$. $M_1$ and $M_2$ are clouds of 3D points obtained from the images took from the perspectives before and after a rotation, defined as $R$. Hence, the function to minimize is
\begin{equation}
\label{sec2:eq:ptrace}
\left \| M_2 - RM_1 \right \|^2 
\end{equation}
that through the Frobenius norm can be expanded as
\begin{equation}
\label{sec2:eq:ptrace}
\left \| M_2 - RM_1 \right \|^2 = \operatorname { trace } ( M_2^T M_2 + M_1^T M_1) - 2 \operatorname { trace } (M_2^T R M_1).
\end{equation}
This means that minimizing (\ref{sec2:eq:procrustes}) with respect to $R$ is equivalent to maximizing the second term of (\ref{sec2:eq:ptrace}). By applying \gls{svd} (SVD) to $M_1 M_2^T$, the latter term can be further simplified into
\begin{equation}
\label{sec2:eq:svd}
\operatorname { trace } (M_2^T R M_1) = \operatorname { trace } (M_1 M_2^T  R) = \operatorname { trace } ( U \Sigma V^T R) = \operatorname { trace } (\Sigma V^T R U) = \operatorname { trace } (\Sigma H )  = \sum _ { i = 1 } ^ { N } \sigma _ { i }h _ { i i } .
\end{equation}
The singular values of $\sigma _ { i }$ are all non-negative, and so the expression becomes maximum when $h _ { i i } = 1$ for $i=1,2,...,N$, since $H$, a product of orthogonal matrices, is an orthogonal matrix itself, thus having its maximal value when $H = I$. This results in $I = V^T R U$, obtaining $R=V U^T$ as the rotation of the eye.   

When $M_1$ and $M_2$ are associated with different origins, it is customary to consider  better fits of the configurations by allowing shape-preserving translations of the origin.
When translating the points clouds by $t_1$ and $t_2$, respectively, (\ref{sec2:eq:procrustes}) becomes
\begin{equation}
\label{sec2:eq:strans1}
\left \|(M_2 - \mathbf{1} \mathbf{t_2}^T) - R(M_1 - \mathbf{1} \mathbf{t_1}^T)\right \|^2,
\end{equation}
which can also be written as
\begin{equation}
\label{sec2:eq:strans2}
\left \| M_2  - RM_1 - \mathbf{1} \mathbf{t}^T\right \|^2,
\end{equation}
where $\mathbf{t}^T = \mathbf{t_2}^T - R\mathbf{t_1}^T$. The expression is minimized when $\mathbf{t}^T$ corresponds to the column-means of $M_2 - RM_1$, therefore a simple way of effecting this translation is to remove the column-means of $M_1$ and $M_2$, separately, before minimizing, which is called centering. The data then becomes expressed in terms of the mean deviations. \cite{procrustes} 

However, in order to obtain the 3D point clouds from the \acrshort{rgb} images, taken before and after rotation, it is necessary to have the depth $\lambda$ that corresponds to the $Z$ coordinate in the real world, as seen on section \ref{cha2:cameramodel}. Assuming pure rotation, any depth provides the same information, so it's possible to define an artificial depth by projecting the 2D points in a sphere with enough radius compared to the focal length, as follows
\begin{equation}
	\label{cha2:sec3:eq:spherepro}
	\left \| (\lambda \mathbf{m_1'})  \right \| = \left \| (\lambda x_1', \lambda y_1', \lambda)  \right \| = r
\end{equation}
where $r$ is the radius of the sphere and $\lambda$ then becomes
\begin{equation}
	\label{cha2:sec3:eq:lamdba}
	\lambda = \frac{r}{\sqrt{x'^2 + y'^2 + 1}},.
\end{equation}
Yet, this makes centering inapplicable. Thus to use this algorithm, the translation associated to the eye movement in the current eye prototype has to be ignored, making the rotation derived from \acrshort{oppr} a mere approximate estimation.
